{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Nanodegree\n",
    "\n",
    "### Capstone Project\n",
    "---\n",
    "\n",
    "## Stock Price Prediction\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Read in the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 60\n",
    "# import all libraries\n",
    "import io\n",
    "import os\n",
    "import time, requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -r requirements.txt --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session, role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/home/ec2-user/SageMaker/ML_Stock_Price_Prediction'\n",
    "os.chdir(source_dir)\n",
    "\n",
    "data_dir = source_dir + '/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "processed_data_dir = source_dir +'/data/processedData'\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'GOOGL'\n",
    "start  = '2009-11-01'\n",
    "end    = '2019-11-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# def get_epoch_time(date):\n",
    "#     ''' Get the epoch time for a particular date\n",
    "#         param | date: Date in the format YYYY-MM-DD\n",
    "#     '''\n",
    "#     os.environ['TZ']='EST+5ETD'\n",
    "#     date_time = date + ' 00:00:00'\n",
    "#     pattern = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "#     epoch = int(time.mktime(time.strptime(date_time, pattern)))\n",
    "#     return epoch\n",
    "\n",
    "def get_historical_data(symbol,start_date,end_date, data_dir):\n",
    "    ''' Daily quotes from Google. Date format='yyyy-mm-dd' '''\n",
    "    symbol = symbol.upper()\n",
    "    df = yf.download('{0}'.format(symbol), start=start_date, end=end_date)\n",
    "    df.to_csv(data_dir+'{0}.csv'.format(symbol), index = True, header=False)\n",
    "    col_names = ['Date','Open','High','Low','Close','Adj_Close','Volume']\n",
    "    stocks = pd.read_csv('data/{0}.csv'.format(symbol), header=0, names=col_names)\n",
    "    \n",
    "    dataFile = pd.DataFrame(stocks)\n",
    "    return dataFile\n",
    "    \n",
    "#     col_names = ['Date','Open','High','Low','Close','Volume']\n",
    "#     stocks = pd.read_csv(url_string, header=0, names=col_names) \n",
    "    \n",
    "#     df = pd.DataFrame(stocks)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_historical_data(symbol, start, end, data_dir) # from January 1, 2005 to June 30, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import viz_functions as viz, processData as ppd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ppd.remove_data(data)\n",
    "\n",
    "#Print the dataframe head and tail\n",
    "print(stocks.head())\n",
    "print(\"---\")\n",
    "print(stocks.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_basic(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ppd.normalise_data(stocks)\n",
    "print(stocks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_basic(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(processed_data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.to_csv(processed_data_dir + '/{0}_processed.csv'.format(symbol) ,index= False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearLearner\n",
    "from sagemaker import LinearLearner\n",
    "\n",
    "# specify an output path\n",
    "prefix = 'stockPrices'\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "# instantiate LinearLearner\n",
    "linear = LinearLearner(role=role,\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.c4.xlarge',\n",
    "                       predictor_type='regressor',\n",
    "                       loss='squared_loss',\n",
    "                       output_path=output_path,\n",
    "                       sagemaker_session=sagemaker_session,\n",
    "                       epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_csv(processed_data_dir + '/{0}_processed.csv'.format(symbol))\n",
    "display(stocks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "X_train, X_test, y_train, y_test = ppd.train_test_split(stocks, train_fraction)\n",
    "\n",
    "print(\"x_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features/labels to numpy in float32 format\n",
    "train_x_np = X_train.astype('float32')\n",
    "train_y_np = y_train.astype('float32')\n",
    "\n",
    "print(\"y_train\", train_y_np.shape)\n",
    "\n",
    "# create RecordSet\n",
    "formatted_train_data = linear.record_set(train_x_np , labels=train_y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# train the estimator on formatted training data\n",
    "linear.fit(formatted_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features/labels to numpy\n",
    "test_x_np = X_test.astype('float32')\n",
    "test_y_np = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# deploy and create a predictor\n",
    "linear_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "result = linear_predictor.predict(test_x_np[0])\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# code to evaluate the endpoint on test data\n",
    "# returns a variety of model metrics\n",
    "def evaluate(predictor, test_features, test_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We have a lot of test data, so we'll split it into batches of 100\n",
    "    # split the test data set into batches and evaluate using prediction endpoint    \n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(test_features, 100)]\n",
    "    \n",
    "    # LinearLearner produces a `predicted_label` for each data point in a batch\n",
    "    # get the 'predicted_label' for every point in a batch\n",
    "    test_preds = np.concatenate([np.array([x.label['score'].float32_tensor.values[0] for x in batch]) \n",
    "                                 for batch in prediction_batches])\n",
    "    \n",
    "    viz.plot_prediction(test_labels, test_preds)\n",
    "\n",
    "    #calculate the score based on mean squared error\n",
    "    score = mean_squared_error(test_preds, test_labels)\n",
    "    \n",
    "    # printing a table of metrics\n",
    "    if verbose:\n",
    "        print(\"{:<11} {:.8f} MSE ({:.8f} RMSE)\".format('Score:', score, math.sqrt(score)))\n",
    "        print()\n",
    "        \n",
    "    return {'Predictions': test_preds, 'Score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics for simple, LinearLearner.\\n')\n",
    "\n",
    "# get metrics for linear predictor\n",
    "predictions, metrics = evaluate(linear_predictor, \n",
    "                   test_x_np, \n",
    "                   test_y_np, \n",
    "                   verbose=True) # verbose means we'll print out the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes a precictor.endpoint\n",
    "def delete_endpoint(predictor):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(linear_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_csv(processed_data_dir + '/{0}_processed.csv'.format(symbol))\n",
    "display(stocks.head())\n",
    "\n",
    "display(stocks.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "val_fraction = 0.2\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = ppd.train_test_split(stocks, \n",
    "                                                                      train_fraction, \n",
    "                                                                      val_frac=val_fraction)\n",
    "\n",
    "print(\"x_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_validation\", X_val.shape)\n",
    "print(\"y_validation\", y_val.shape)\n",
    "print(\"x_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "X_test.to_csv(os.path.join(processed_data_dir, 'XGB_features_test.csv'), header=False, index=False)\n",
    "y_test.to_csv(os.path.join(processed_data_dir, 'XGB_labels_test.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(y_val), pd.DataFrame(X_val)], axis=1).to_csv(os.path.join(processed_data_dir, 'XGB_validation.csv'), header=False, index=False)\n",
    "pd.concat([pd.DataFrame(y_train), pd.DataFrame(X_train)], axis=1).to_csv(os.path.join(processed_data_dir, 'XGB_train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'stockPrices-xgboost'\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "test_location = sagemaker_session.upload_data(os.path.join(processed_data_dir, 'XGB_features_test.csv'), key_prefix=prefix)\n",
    "val_location = sagemaker_session.upload_data(os.path.join(processed_data_dir, 'XGB_validation.csv'), key_prefix=prefix)\n",
    "train_location = sagemaker_session.upload_data(os.path.join(processed_data_dir, 'XGB_train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated above, we use this utility method to construct the image name for the training container.\n",
    "container = get_image_uri(sagemaker_session.boto_region_name, 'xgboost')\n",
    "\n",
    "# Now that we know which container to use, we can construct the estimator object.\n",
    "xgb = sagemaker.estimator.Estimator(container,     # The name of the training container\n",
    "                                    role = role,             # The IAM role to use (our current role in this case)\n",
    "                                    train_instance_count=1,  # The number of instances to use for training\n",
    "                                    train_instance_type='ml.m4.xlarge', # The type of instance ot use for training\n",
    "                                    output_path= output_path,  # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=sagemaker_session) # The current SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(base_score=0.5, \n",
    "                        booster='gbtree',\n",
    "                        max_depth=3,\n",
    "                        eta=0.1,\n",
    "                        gamma=0,\n",
    "                        alpha=0,\n",
    "                        min_child_weight=13,\n",
    "                        subsample=1.0,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is a wrapper around the location of our train and validation data, to make sure that SageMaker\n",
    "# knows our data is in csv format.\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp --recursive $xgb_transformer.output_path $processed_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_pred = pd.read_csv(os.path.join(processed_data_dir, 'XGB_features_test.csv.out'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tell the endpoint what format the data we are sending is in\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "\n",
    "predictions = xgb_predictor.predict(X_test)\n",
    "# predictions = xgb_predictor.predict(X_test.values).decode('utf-8')\n",
    "# predictions is currently a comma delimited string and so we would like to break it up\n",
    "# as a numpy array.\n",
    "predictions = np.fromstring(predictions, sep=',')\n",
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_prediction(y_test, Y_pred)\n",
    "# plt.scatter(y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "score = mean_squared_error(y_test, predictions)\n",
    "print(\"{:<11} {:.8f} MSE ({:.8f} RMSE)\".format('Score:', score, math.sqrt(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(xgb_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM) Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_csv(processed_data_dir + '/{0}_processed.csv'.format(symbol))\n",
    "display(stocks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "val_fraction = 0.2\n",
    "\n",
    "history_size = 200\n",
    "target_size = 14\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = ppd.create_dataset_lstm(stocks, \n",
    "                                                                         train_frac=train_fraction, \n",
    "                                                                         history_size=history_size, \n",
    "                                                                         target_size=target_size,\n",
    "                                                                         val_frac=val_fraction)\n",
    "\n",
    "X_train = X_train.reshape((-1, history_size, 1))\n",
    "y_train = y_train.reshape((-1, target_size))\n",
    "\n",
    "X_val = X_val.reshape((-1, history_size, 1))\n",
    "y_val = y_val.reshape((-1, target_size))\n",
    "\n",
    "print(\"x_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_validation\", X_val.shape)\n",
    "print(\"y_validation\", y_val.shape)\n",
    "print(\"x_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lstm.lstm_model import build_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = build_lstm_model(input_dim = X_train.shape[2], output_dim = target_size, return_sequences=True)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=100,\n",
    "          batch_size = history_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((-1, history_size, 1))\n",
    "y_test = y_test.reshape((-1, target_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_prediction(y_test[:,4],predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data_dir directory\n",
    "# And then we delete the directory itself\n",
    "%rm -rf $processed_data_dir/*\n",
    "%rm -rf $processed_data_dir\n",
    "\n",
    "%rm -rf $data_dir/*\n",
    "%rm -rf $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
